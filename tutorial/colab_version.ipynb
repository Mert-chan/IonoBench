{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4188ee",
   "metadata": {},
   "source": [
    "Dear User,\n",
    "\n",
    "This is a tutorial script showcasing the testing modules of the IonoBench Framework. It also serves as an introductory documentation script for the framework for you to download datasets and trained models.\n",
    "\n",
    "**Instructions** \n",
    "\n",
    "You can download the provided dataset and pretrained models, then run the test modules to replicate paper results.\n",
    "\n",
    "This notebook is designed for **Google Colab** and requires a **GPU runtime**. (However if you build locally you can still use VS code notebook or Jupyter etc. to run the code cells.)\n",
    "To enable GPU: click on the `down arrow` **right** beside `Connect` **in the top-right**, then go to `Change runtime type > Hardware accelerator > GPU (T4 GPU etc.)`.\n",
    "\n",
    "Please run the notebook **step-by-step**, following the comments provided.\n",
    "\n",
    "There are three main testing sections:\n",
    "- **3b. Default Test**  \n",
    "- **4a. Solar Analysis**  \n",
    "- **4b. Storm Analysis**\n",
    "\n",
    "Both the default test and solar analysis are commented out for your convenience, as each may take ~20 minutes to run (on Colab GPU). If you prefer a quicker test, you can leave them commented and run **Storm Analysis** section, which completes in a few minutes.\n",
    "\n",
    "After that section you can observe the paper results on `5.3.3. Visual Comparison: Residual Patterns during Stormy vs. Quiet Conditions` (Specifically first two rows: stormy example.)\n",
    "If you wait for other test runs you can reproduce the paper results `5.1. Overall Performance` and `5.2. Performance Across Solar Activity Levels`\n",
    "\n",
    "Start with the default model “SimVPv2.”\n",
    "If you want to **test other models**, just restart the notebook. Make sure you;\n",
    "- Download the new model weights from `2: Models and Configs` and change the path on `3a: Loading Pre-trained Model`.\n",
    "- Change the model name in the configs.\n",
    "- Give each run a unique session name so the results are stored separately. before you start the test parts.\n",
    "\n",
    "**Colab Issues (You might face..)**\n",
    "- **Session crash:** On the upper left panel find `Runtime` and select `Restart Session`. (Good news: In the second run you won't need to wait for downloads)\n",
    "- **NVIDIA driver error:** Check if the GPU is correctly shown in the `# 0: Preps >>> Verify GPU session`.\n",
    "- **Timeout:** Colab sessions are limited; you may need to restart and rerun.\n",
    "- **Download errors:** If dataset or model download fails due to interruption, delete any existing `datasets/` or `training_sessions/` folders from the loaded git folder and try again.\n",
    "\n",
    "**Note: All repeating cells used for data preparation before different testing types will be hidden using CLI support. Currently this notebook is showing how the underlying config structure and functions works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320a3c0",
   "metadata": {},
   "source": [
    "--- \n",
    "## 0: Preps  \n",
    "Clone repo, install requirements, and verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b3591ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA RTX A5000\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "# 0: Preps >>> Verify GPU session\n",
    "#================================================================\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124aa6e",
   "metadata": {},
   "source": [
    "Please don't continue if `CUDA available: False` and select GPU again and restart session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66985e8a",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Colab Preps (Skip if local build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e64371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Clone Repo (Skip here if you are running locally)\n",
    "#================================================================\n",
    "!git clone https://github.com/Mert-chan/IonoBench --quiet\n",
    "%cd IonoBench\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Download the required libs\n",
    "#================================================================\n",
    "!pip install -r /content/IonoBench/requirements.txt --quiet              # Can take couple of minutes\n",
    "!pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Install cu118 Just incase if pytorch can't find \n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21724f",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1: Dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Login to Hugging Face Hub\n",
    "#================================================================\n",
    "from huggingface_hub import login\n",
    "\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66ea3a",
   "metadata": {},
   "source": [
    "If chronological split is desired, change to \"chronological\" However for chronological you can only replicate the Paper Results on  `Section 4 Investigating Future Bias in Stratified Split `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40245fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Download the desired IonoBench dataset (Stratified or Chronological split)\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "import os,sys\n",
    "\n",
    "repo_name = \"DemoRepo\"  \n",
    "base_path = Path(f\"/content/{repo_name}\")\n",
    "sys.path.append(str(base_path))\n",
    "sys.path.append('./source')\n",
    "sys.path.append('./scripts')\n",
    "from source.myDataFuns import download_dataset\n",
    "download_dataset(\n",
    "    dataset_name=\"stratified\",  # If chronological split is desired, change to \"chronological\" However for chronological you can only replicate the Paper Results on Section 4\n",
    "    base_path=base_path\n",
    "    )     \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Read dataset\n",
    "#====================================================================================\n",
    "from scripts.data import load_training_data\n",
    "\n",
    "dataDict = load_training_data(\n",
    "                            seq_len=12,                \n",
    "                            pred_horz=12,\n",
    "                            datasplit='stratified',      # Need to be changed according to the datasplit\n",
    "                            features = None,             # Default \"None\" loads all features, otherwise specify a list of features i.e ['F10.7', 'Dst']\n",
    "                            base_path=base_path\n",
    "                            )        \n",
    "# You can also see the full list of features in ~/configs/base.yaml\n",
    "\n",
    "print(\"\\n\", \"-\" * 20, \"\\n\", dataDict.keys())\n",
    "print(\"You can use the dataDict following dict_keys as you like from this point on.\")\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features\n",
    "#====================================================================================\n",
    "dataDict['OMNI_names']\n",
    "#====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features (note: all features are normalized to 0-1) \n",
    "#=====================================================================================\n",
    "'''\n",
    "\"normOMNI\" is same order with the \"OMNI_names\"\n",
    "Check the name and access the desired feature using the getOMNIfeature function.\n",
    "'''\n",
    "from source.myDataFuns import getOMNIfeature\n",
    "\n",
    "# To access 'Dst' values \n",
    "print(\"Dst:\", getOMNIfeature(dataDict,\"Dst\")) # Dst values\n",
    "print(\"Year:\", getOMNIfeature(dataDict,\"Year\")) # Year values\n",
    "\n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing TEC data for specific date range (note: tec data is normalized to 0-1)\n",
    "#======================================================================================\n",
    "from datetime import datetime, timedelta\n",
    "from source.myDataFuns import dateList\n",
    "# Select the date period\n",
    "startDate = datetime(2024, 5, 9, 0, 0)  # Start date\n",
    "endDate = datetime(2024, 5, 9, 22, 0)    # End date (inclusive)\n",
    "\n",
    "# Find idx corresponding to start and end dates\n",
    "startidx = dataDict['dates'].index(startDate)\n",
    "endidx = dataDict['dates'].index(endDate)\n",
    "\n",
    "# Extract TEC data for the selected date range\n",
    "norm_tecData = dataDict['normTEC'][startidx:endidx+1]\n",
    "date_list = dateList(startDate, endDate,timedelta(hours=2)) # TEC maps are 2 hours apart\n",
    "print(\"TEC data shape for the selected date range:\", norm_tecData.shape)\n",
    "print(\"Dates of the selected date range:\", len(date_list))\n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Reversing the Preprocessing Steps and Visualizing the Original TEC Data\n",
    "#======================================================================================\n",
    "'''\n",
    "To recover original TEC data, the preprocessing steps (Normalization and heliocentric transformation) needed to be reversed. \n",
    "'''\n",
    "from source.myDataFuns import reverseHeliocentric\n",
    "tecData = norm_tecData*(dataDict['maxTEC'] - dataDict['minTEC']) + dataDict['minTEC']  # Reverse normalization\n",
    "org_tecData = reverseHeliocentric(tecData, date_list)\n",
    "\n",
    "from source.myVisualFuns import makeComparison_Anim\n",
    "from IPython.display import HTML\n",
    "# Define the plot titles for the animation\n",
    "plot_titles = {\n",
    "    'main': 'Reverse Heliocentric Transformation',\n",
    "    'subplot1': 'Real VTEC IGS',\n",
    "    'subplot2': 'Longitude-Shifted VTEC IGS',\n",
    "    'colorbar': 'TECU (10$^{16}$ e/m$^2$)'\n",
    "}\n",
    "\n",
    "# Create a comparison animation of the original and transformed TEC data\n",
    "'''\n",
    "This is for demonstration purposes. Reversing the effect of the heliocentric transformation is shown in below animation\n",
    "Right side shows after reverse heliocentric transformation, left side shows dataset TEC data before transformation.\n",
    "'''\n",
    "anim = makeComparison_Anim(\n",
    "    data1=org_tecData,\n",
    "    data2=tecData,\n",
    "    date_list=date_list,\n",
    "    titles=plot_titles,\n",
    "    show_metrics=False\n",
    ")\n",
    "HTML(anim.to_jshtml())  \n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b00ee",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2: Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8601b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Download the desired Trained Model (SimVP2,SwinLSTM, DCNN etc.) from Hugging Face Hub\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "from source.myDataFuns import download_model_folder\n",
    "\n",
    "data_path = Path(base_path, \"training_sessions\")         # ..~/DemoRepo/training_sessions \n",
    "data_path.mkdir(parents=True, exist_ok=True)             # Create new folder when model folder is downloaded.\n",
    "\n",
    "                          #\n",
    "download_model_folder(                                   # This function automatically downloads the paper's model folder from Hugging Face Hub.\n",
    "                    model_name = \"SimVPv2\",              # Change to \"DCNN\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                    base_path = base_path\n",
    "                    )    \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Load the model configurations\n",
    "#==================================================================================\n",
    "from scripts.loadConfigs import load_configs\n",
    "\n",
    "'''\n",
    "Configs uses base => model => mode => CLI override hierarchy.\n",
    "You can change the model and mode to load different configurations.\n",
    "For example, change \"SimVPv2\" to \"DCNN121\", \"SwinLSTM\" etc. to load different model configurations.\n",
    "load_configs will return a merged configurations of the base, model, and mode.\n",
    "'''\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # Change to \"DCNN121\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                 mode = \"test\",         \n",
    "                 split = \"stratified\",  \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "#=================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ceca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Can check the keys of the config dictionary\n",
    "#================================================================\n",
    "print(cfgs.keys())\n",
    "print(cfgs.data.keys())\n",
    "print(cfgs.model.keys())\n",
    "print(cfgs.data.data_split)\n",
    "print(cfgs.model)\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795a207",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3: Testing Trained Models \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fa3bb",
   "metadata": {},
   "source": [
    "\n",
    "### 3a: Loading Pre-trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021206b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model>>> Prepare the data and build the model     \n",
    "#=================================================================\n",
    "\n",
    "from scripts.registry import build_model\n",
    "from scripts.data import prepare_raw\n",
    "\n",
    "# Prepare the data for testing\n",
    "data = prepare_raw(cfgs)               # This fun Wraps load_training_data & patches cfg with shape/min-max info.\n",
    "B = cfgs.test.batch_size               # batch size from YAML\n",
    "T = cfgs.data.seq_len                  # input sequence length\n",
    "C = cfgs.data.num_omni + 1             # OMNI scalars + TEC map channel\n",
    "H = cfgs.data.H                        # height (set inside prepare_raw)\n",
    "W = cfgs.data.W                        # width  (set inside prepare_raw)\n",
    "cfgs.test.input_names = data['OMNI_names']          # This sets the input names for test logging file\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise CPU\n",
    "# Build the model\n",
    "cfgs.model.input_shape = (T, C, H, W)  # Set input shape for the model\n",
    "model = build_model(cfg = cfgs, base_path=base_path, device=device)           # Build the model with the configurations\n",
    "print(f\"Batch size: {B}, T: {T}, C: {C}, H: {H}, W: {W}\")\n",
    "#=================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Summary of the model   \n",
    "#=================================================================\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=((B, C, T, H, W),(B, cfgs.data.pred_horz, H, W)))\n",
    "#================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e83f4",
   "metadata": {},
   "source": [
    "\n",
    "!!!! **Before loading the weights to build model**  Change the Path of **torch.load** to the path of your downloaded model checkpoint folder if you are trying DCNN or SwinLSTM. (Default is set to SimVPv2)\n",
    "You can find the checkpoint file in the downloaded model folder under `training_sessions/{modelName}` as .... `\"NameofTheSession\"_best_checkpoint_\"yyyymmdd\"_\"hhmm\" `\n",
    "To download new pre-trained model, refer back to the `2: Models and Configs >>> Download the desired Trained Model` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Load the model weights from a checkpoint file\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import DDPtoSingleGPU\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    r'/content/DemoRepo/training_sessions/SimVPv2/SimVP_stratifiedSplit_Allfeatures_best_checkpoint_20250320_1401.pth',            # <= Copy inside\n",
    "    weights_only=True)\n",
    "model.load_state_dict(DDPtoSingleGPU(checkpoint[\"model_state_dict\"]))   # Load the model state dict If DDP(Multiple GPU) was used get rid of Module prefix\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913f21",
   "metadata": {},
   "source": [
    "--- \n",
    "### 3b: Default Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa314fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**!!! Disclaimer !!!**  \n",
    "The paper’s experiments were primarily trained and tested using **4 GPUs with Distributed Data Parallel (DDP)**.  \n",
    "DDP introduces some non-determinism, even with fixed random seeds, due to differences in data shuffling and the order of floating-point operations across GPUs.\n",
    "\n",
    "This notebook uses a **single GPU**, which may lead to minor variations between the metrics generated here and those reported in the paper\n",
    "\n",
    "These differences are **statistically negligible** and can be validated by comparing with the reported results.  \n",
    "The key point is that these differences will not affect the paper's overall findings or the relative performance ranking of the models.\n",
    "\n",
    "For bit-for-bit reproducibility of your own experiments, it is essential to use a fixed hardware configuration (e.g., the same number of GPUs) for both training and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Setup the data loaders for testing\n",
    "#================================================================\n",
    "from scripts.data import make_default_loaders\n",
    "loaders = make_default_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "len(loaders[\"train\"]), len(loaders[\"valid\"]), len(loaders[\"test\"])\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Test the model (Default: test set) (RUN TIME: >~20 min)\n",
    "#================================================================\n",
    "from source.myTrainFuns import IonoTester\n",
    "\n",
    "# FOR DEFAULT TESTING UNCOMMENT BELOW\n",
    "#================================================================\n",
    "'''\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = False              # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = IonoTester(model, loaders['test'], device=device, config=cfgs).test() \n",
    "'''\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba3a0a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4: Solar and Storm Analysis\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08ca93",
   "metadata": {},
   "source": [
    "\n",
    "### 4a: Solar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88217c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar and Storm Analysis >>> Solar Loaders \n",
    "#====================================================================================\n",
    "from scripts.data import make_solar_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # <= change to \"DCNN121\", \"SwinLSTM\" for other models. (But the loaded model should match the model name)\n",
    "                 mode = \"solar\",     # <= The testing type is set to \"solar\" to load solar configurations dynmaically.\n",
    "                 split = \"stratified\",  \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_solar_loaders(cfg = cfgs, base_path=base_path, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar Analysis >>> Analysis Function (Testing on Solar intensity classes: very weak, weak, moderate, intense) (RUN TIME: >~20 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import SolarAnalysis\n",
    "\n",
    "# FOR SOLAR ANALYSIS UNCOMMENT BELOW\n",
    "#================================================================\n",
    "'''\n",
    "cfgs.session.name = \"SimVPv2_test\"  # Writes on top of the previous test file if the session name is the same.\n",
    "cfgs.test.save_results = True       # Appends the test results to a log file (txt) that exists in the training_sessions folder under the session name. (if not exists, creates a new one) \n",
    "cfgs.test.save_raw = False          # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz. (Per solar class)\n",
    "solarDict = SolarAnalysis(model, data, loaders, device=device, cfg=cfgs).run()\n",
    "'''\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820351b4",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b: Storm Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Storm Loaders\n",
    "#====================================================================================\n",
    "from scripts.data import make_storm_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # <= change to \"DCNN121\", \"SwinLSTM\" for other models. (But the loaded model should match the model name)\n",
    "                 mode = \"storm\",     # <= changed to storm to load storm configurations dynamically.\n",
    "                 split = \"stratified\", \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_storm_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00951699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Analysis Function (Testing on Storm events) (RUN TIME: ~1-3 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import StormAnalysis\n",
    "\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = True               # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = StormAnalysis(model, data, cfgs, loaders,device).run() \n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c0de",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5: Storm Predictions and Residuals\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e2dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf any case I didn't finish this before your review you can run directly to get the results.\\nFirst one shows the single image reported in the paper for Storm #2 \\nFollowing shows a animation of the storm event #2 and SimVPv2's prediction\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Here is raw scripts used for paper I will convert them to functions in the following days\n",
    "'''\n",
    "If any case I didn't finish this before your review you can run directly to get the results.\n",
    "First one shows the single image reported in the paper's section \"5.3.3. Visual Comparison: Residual Patterns during Stormy vs. Quiet Conditions\" \n",
    "Since these are storm periods only first event (Stormy) visualized to show the reproduction relaibly\n",
    "Following cell shows an animation of the same storm event's main phase and SimVPv2's prediction\n",
    "Slight differences are due to different definition of color scales (Paper: 3 Model at the same time, This notebook: 1 Model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "import numpy as np\n",
    "npz_path = base_path / \"training_sessions\" / cfgs.session.name / \"storms\" / \"test_raw_storm_2_2001-11-06.npz\"\n",
    "data_npz = np.load(npz_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> TO DO: Make this a function\n",
    "from source.myDataFuns import reverseHeliocentricSingle\n",
    "'''\n",
    "To recover original TEC data the preprocessing steps (Normalization and heliocentric transformation) needed to be reversed. \n",
    "'''\n",
    "storm_metadata = data_npz['storm_metadata'].item()  # Unwrap the dictionary\n",
    "n = 36              # chosen sample index\n",
    "h = 6               # 12h ahead\n",
    "selectedDate = storm_metadata['Storm Period'][n]            # Date of interest\n",
    "preds = data_npz['full_period_predictions'][n+(12-h)][h]    # Predictions for the selected date (Need to be offset according to horizon)\n",
    "labels = data_npz['full_period_labels'][n+(12-h)][h]        # Labels for the selected date\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "pred = reverseHeliocentricSingle(np.array(preds), selectedDate)\n",
    "label = reverseHeliocentricSingle(np.array(labels), selectedDate)\n",
    "residual_data = pred - label\n",
    "\n",
    "# 3. Calculate All Necessary Metrics & Statistics\n",
    "rmse = np.sqrt(np.mean(residual_data ** 2))\n",
    "r2 = r2_score(label.ravel(), pred.ravel())\n",
    "ssim_val = ssim(label, pred, data_range=dataDict['maxTEC'] - dataDict['minTEC'])\n",
    "res_min, res_max, res_std = np.min(residual_data), np.max(residual_data), np.std(residual_data)\n",
    "\n",
    "# 4. Setup for Cartopy Plotting\n",
    "lat_size, lon_size = pred.shape\n",
    "lon = np.linspace(-180, 180, lon_size, endpoint=False)\n",
    "lat = np.linspace(90, -90, lat_size)\n",
    "Lon, Lat = np.meshgrid(lon, lat)\n",
    "\n",
    "# Color scale limits for the plots\n",
    "vmin_data, vmax_data = np.min(label), np.max(label)\n",
    "max_abs_res = np.max(np.abs(residual_data))\n",
    "plot_data = [label, pred, residual_data]\n",
    "\n",
    "plt.style.use('dark_background') # Apply the dark theme\n",
    "plt.rcParams.update({'font.size': 15}) # Set base font size\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1, 3,\n",
    "    figsize=(20, 5), # Adjusted figsize for better proportions\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()}, \n",
    "    gridspec_kw={'wspace': 0.3, 'hspace': 0.05}\n",
    ")\n",
    "\n",
    "# Define titles with metrics\n",
    "titles = [\n",
    "    f'Real VTEC IGS\\n{selectedDate:%Y-%m-%d %H:%M}',\n",
    "    f'{cfgs.session.name} Prediction\\nRMSE={rmse:.2f}  R²={r2:.2f}  SSIM={ssim_val:.2f}',\n",
    "    f'{cfgs.session.name} Residual\\nMax: {res_max:.2f} Min: {res_min:.2f} Std: {res_std:.2f}'\n",
    "]\n",
    "\n",
    "# 5. Plotting on each map\n",
    "# Plot 1: Real Data\n",
    "im_data = axs[0].pcolormesh(Lon, Lat, plot_data[0], cmap='jet', vmin=vmin_data, vmax=vmax_data, shading='auto', transform=ccrs.PlateCarree())\n",
    "# Plot 2: Prediction\n",
    "axs[1].pcolormesh(Lon, Lat, plot_data[1], cmap='jet', vmin=vmin_data, vmax=vmax_data, shading='auto', transform=ccrs.PlateCarree())\n",
    "# Plot 3: Residuals\n",
    "im_res = axs[2].pcolormesh(Lon, Lat, plot_data[2], cmap='bwr', vmin=-max_abs_res, vmax=max_abs_res, shading='auto', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add styled contours to the residual plot\n",
    "cs = axs[2].contour(Lon, Lat, plot_data[2], levels=6, colors='black', linewidths=0.8, linestyles='dotted', transform=ccrs.PlateCarree())\n",
    "axs[2].clabel(cs, inline=True, fontsize=9, fmt='%.1f')\n",
    "\n",
    "# Apply styling from reference script to all maps\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_title(titles[i], pad=10, fontsize=14, fontweight='bold')\n",
    "    ax.set_global()\n",
    "    ax.coastlines(resolution='110m', linewidth=1.0)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.8, linestyle=':')\n",
    "    ax.gridlines(draw_labels=False, color='gray', linewidth=0.75, alpha=0.3)\n",
    "\n",
    "    # Set styled ticks and labels\n",
    "    ax.set_xticks(np.arange(-180, 181, 60), crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(np.arange(-90, 91, 30), crs=ccrs.PlateCarree())\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}°'))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{int(y)}°' if i == 0 else '')) # Y-labels only on first plot\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    # Make tick labels bold\n",
    "    for tl in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        tl.set_fontweight('bold')\n",
    "\n",
    "# 6. Manage Colorbars with the target style\n",
    "# Colorbar for Data\n",
    "cb_ax1 = inset_axes(axs[1], width=\"4%\", height=\"110%\", loc='center left',\n",
    "                    bbox_to_anchor=(1.05, -0.05, 1, 1.1), bbox_transform=axs[1].transAxes)\n",
    "cb1 = fig.colorbar(im_data, cax=cb_ax1, orientation='vertical')\n",
    "cb1.set_label(\"TECU (10$^{16}$ e/m$^2$)\", fontsize=12, fontweight='bold')\n",
    "cb1.ax.tick_params(labelsize=12)\n",
    "\n",
    "# Colorbar for Residuals\n",
    "cb_ax2 = inset_axes(axs[2], width=\"4%\", height=\"110%\", loc='center left',\n",
    "                    bbox_to_anchor=(1.05, -0.05, 1, 1.1), bbox_transform=axs[2].transAxes)\n",
    "cb2 = fig.colorbar(im_res, cax=cb_ax2, orientation='vertical')\n",
    "cb2.set_label('Residual TECU', fontsize=12, fontweight='bold')\n",
    "cb2.ax.tick_params(labelsize=12)\n",
    "\n",
    "# 7. Adjust layout and show the plot\n",
    "plt.subplots_adjust(left=0.05, right=0.88, top=0.85, bottom=0.1, wspace=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from source.myDataFuns import reverseHeliocentricSingle\n",
    "\n",
    "# Setup parameters for animation\n",
    "n_start = 33    # Starting storm index\n",
    "n_end = 40      # Ending storm index (adjust based on your data)\n",
    "h = 6           # 12h ahead forecast\n",
    "storm_metadata = data_npz['storm_metadata'].item()\n",
    "\n",
    "# Pre-calculate all data for animation\n",
    "animation_data = []\n",
    "for n in range(n_start, min(n_end, len(storm_metadata['Storm Period']))):\n",
    "    try:\n",
    "        selectedDate = storm_metadata['Storm Period'][n]\n",
    "        preds = data_npz['full_period_predictions'][n + (12 - h)][h]\n",
    "        labels = data_npz['full_period_labels'][n + (12 - h)][h]\n",
    "        \n",
    "        pred = reverseHeliocentricSingle(np.array(preds), selectedDate)\n",
    "        label = reverseHeliocentricSingle(np.array(labels), selectedDate)\n",
    "        residual_data = pred - label\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(np.mean(residual_data ** 2))\n",
    "        r2 = r2_score(label.ravel(), pred.ravel())\n",
    "        ssim_val = ssim(label, pred, data_range=dataDict['maxTEC'] - dataDict['minTEC'])\n",
    "        res_mean, res_max, res_std = np.mean(residual_data), np.max(residual_data), np.std(residual_data)\n",
    "        \n",
    "        animation_data.append({\n",
    "            'date': selectedDate,\n",
    "            'label': label,\n",
    "            'pred': pred,\n",
    "            'residual': residual_data,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'ssim': ssim_val,\n",
    "            'res_mean': res_mean,\n",
    "            'res_max': res_max,\n",
    "            'res_std': res_std\n",
    "        })\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"Skipping frame {n}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Prepared {len(animation_data)} frames for animation\")\n",
    "\n",
    "# Setup coordinate system\n",
    "lat_size, lon_size = animation_data[0]['pred'].shape\n",
    "lon = np.linspace(-180, 180, lon_size, endpoint=False)\n",
    "lat = np.linspace(90, -90, lat_size)\n",
    "Lon, Lat = np.meshgrid(lon, lat)\n",
    "\n",
    "# Determine global color scales across all frames\n",
    "all_labels = [frame['label'] for frame in animation_data]\n",
    "all_preds = [frame['pred'] for frame in animation_data]\n",
    "all_residuals = [frame['residual'] for frame in animation_data]\n",
    "\n",
    "vmin_data = np.min([np.min(data) for data in all_labels + all_preds])\n",
    "vmax_data = np.max([np.max(data) for data in all_labels + all_preds])\n",
    "max_abs_res = np.max([np.max(np.abs(data)) for data in all_residuals])\n",
    "\n",
    "# Setup the figure and axes\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    1, 3,\n",
    "    figsize=(20, 5),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "    gridspec_kw={'wspace': 0.3, 'hspace': 0.05}\n",
    ")\n",
    "\n",
    "# Initialize empty plot objects\n",
    "im_data = None\n",
    "im_pred = None\n",
    "im_res = None\n",
    "contour_lines = None\n",
    "\n",
    "# Setup axes styling \n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_global()\n",
    "    ax.coastlines(resolution='110m', linewidth=1.0)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.8, linestyle=':')\n",
    "    ax.gridlines(draw_labels=False, color='gray', linewidth=0.75, alpha=0.3)\n",
    "    \n",
    "    ax.set_xticks(np.arange(-180, 181, 60), crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(np.arange(-90, 91, 30), crs=ccrs.PlateCarree())\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}°'))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{int(y)}°' if i == 0 else ''))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    \n",
    "    for tl in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        tl.set_fontweight('bold')\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialize the animation\"\"\"\n",
    "    return []\n",
    "\n",
    "def update(frame_idx):\n",
    "    \"\"\"Update function for each animation frame\"\"\"\n",
    "    global im_data, im_pred, im_res, contour_lines\n",
    "    \n",
    "    # Get current frame data\n",
    "    frame = animation_data[frame_idx]\n",
    "    \n",
    "    # Clear previous plots\n",
    "    for ax in axs:\n",
    "        ax.clear()\n",
    "        # Re-setup axes styling\n",
    "        ax.set_global()\n",
    "        ax.coastlines(resolution='110m', linewidth=1.0)\n",
    "        ax.add_feature(cfeature.BORDERS, linewidth=0.8, linestyle=':')\n",
    "        ax.gridlines(draw_labels=False, color='gray', linewidth=0.75, alpha=0.3)\n",
    "        \n",
    "        ax.set_xticks(np.arange(-180, 181, 60), crs=ccrs.PlateCarree())\n",
    "        ax.set_yticks(np.arange(-90, 91, 30), crs=ccrs.PlateCarree())\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x)}°'))\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{int(y)}°' if ax == axs[0] else ''))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "        \n",
    "        for tl in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "            tl.set_fontweight('bold')\n",
    "    \n",
    "    # Plot data\n",
    "    im_data = axs[0].pcolormesh(Lon, Lat, frame['label'], \n",
    "                               cmap='jet', vmin=vmin_data, vmax=vmax_data, \n",
    "                               shading='auto', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    im_pred = axs[1].pcolormesh(Lon, Lat, frame['pred'], \n",
    "                               cmap='jet', vmin=vmin_data, vmax=vmax_data, \n",
    "                               shading='auto', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    im_res = axs[2].pcolormesh(Lon, Lat, frame['residual'], \n",
    "                              cmap='bwr', vmin=-max_abs_res, vmax=max_abs_res, \n",
    "                              shading='auto', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add contours to residual plot\n",
    "    try:\n",
    "        cs = axs[2].contour(Lon, Lat, frame['residual'], levels=6, \n",
    "                           colors='black', linewidths=0.8, linestyles='dotted', \n",
    "                           transform=ccrs.PlateCarree())\n",
    "        axs[2].clabel(cs, inline=True, fontsize=9, fmt='%.1f')\n",
    "    except:\n",
    "        pass  # Skip contours if they cause issues\n",
    "    \n",
    "    titles = [\n",
    "        f'Real VTEC IGS\\n{frame[\"date\"]:%Y-%m-%d %H:%M}',\n",
    "        f'{cfgs.session.name} Prediction\\nRMSE={frame[\"rmse\"]:.2f}  R²={frame[\"r2\"]:.2f}  SSIM={frame[\"ssim\"]:.2f}',\n",
    "        f'{cfgs.session.name} Residual\\nMax: {frame[\"res_max\"]:.2f} Mean: {frame[\"res_mean\"]:.2f} Std: {frame[\"res_std\"]:.2f}'\n",
    "    ]\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.set_title(titles[i], pad=10, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    return [im_data, im_pred, im_res]\n",
    "\n",
    "# Create the animation\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, update, init_func=init,\n",
    "    frames=len(animation_data), interval=300, \n",
    "    blit=False, repeat=True\n",
    ")\n",
    "if len(animation_data) > 0:\n",
    "    # Create initial plot to get colorbar reference\n",
    "    initial_frame = animation_data[0]\n",
    "    im_data = axs[0].pcolormesh(Lon, Lat, initial_frame['label'], \n",
    "                               cmap='jet', vmin=vmin_data, vmax=vmax_data, \n",
    "                               shading='auto', transform=ccrs.PlateCarree())\n",
    "    im_res = axs[2].pcolormesh(Lon, Lat, initial_frame['residual'], \n",
    "                              cmap='bwr', vmin=-max_abs_res, vmax=max_abs_res, \n",
    "                              shading='auto', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add colorbars\n",
    "    cb_ax1 = inset_axes(axs[1], width=\"4%\", height=\"110%\", loc='center left',\n",
    "                        bbox_to_anchor=(1.05, -0.05, 1, 1.1), bbox_transform=axs[1].transAxes)\n",
    "    cb1 = fig.colorbar(im_data, cax=cb_ax1, orientation='vertical')\n",
    "    cb1.set_label(\"TECU (10$^{16}$ e/m$^2$)\", fontsize=12, fontweight='bold')\n",
    "    cb1.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    cb_ax2 = inset_axes(axs[2], width=\"4%\", height=\"110%\", loc='center left',\n",
    "                        bbox_to_anchor=(1.05, -0.05, 1, 1.1), bbox_transform=axs[2].transAxes)\n",
    "    cb2 = fig.colorbar(im_res, cax=cb_ax2, orientation='vertical')\n",
    "    cb2.set_label('Residual TECU', fontsize=12, fontweight='bold')\n",
    "    cb2.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.90, top=0.95, bottom=0.05, wspace=0.15)\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ionobench_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
