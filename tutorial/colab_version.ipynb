{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4188ee",
   "metadata": {},
   "source": [
    "Dear User,\n",
    "\n",
    "This is a tutorial script showcasing the testing modules of the IonoBench Framework. It also serves as an introductory documentation script for the framework for you to download datasets and trained models.\n",
    "\n",
    "**Instructions** \n",
    "\n",
    "You can download the provided dataset and pretrained models, then run the test modules to replicate paper results.\n",
    "\n",
    "This notebook is designed for **Google Colab** and requires a **GPU runtime**.\n",
    "To enable GPU: click on the `down arrow` **right** beside `Connect` **in the top-right**, then go to `Change runtime type > Hardware accelerator > GPU (T4 GPU etc.)`.\n",
    "\n",
    "Please run the notebook **step-by-step**, following the comments provided.\n",
    "\n",
    "There are three main testing sections:\n",
    "- **3b. Default Test**  \n",
    "- **4a. Solar Analysis**  \n",
    "- **4b. Storm Analysis**\n",
    "\n",
    "Both the default test and solar analysis are commented out for your convenience, as each may take ~20 minutes to run (on Colab GPU). If you prefer a quicker test, you can leave them commented and run **Storm Analysis** section, which completes in a few minutes.\n",
    "\n",
    "After that section you can observe the paper results on `5.3.3. Visual Comparison: Residual Patterns during Stormy vs. Quiet Conditions` (Specifically first two rows: stormy example.)\n",
    "If you wait for other test runs you can reproduce the paper results `5.1. Overall Performance` and `5.2. Performance Across Solar Activity Levels`\n",
    "\n",
    "Start with the default model “SimVPv2.”\n",
    "If you want to **test other models**, just restart the notebook. Make sure you;\n",
    "- Download the new model weights from `2: Models and Configs` and change the path on `3a: Loading Pre-trained Model`.\n",
    "- Change the model name in the configs.\n",
    "- Give each run a unique session name so the results are stored separately. before you start the test parts.\n",
    "\n",
    "**Common Issues You Might Face**\n",
    "- **Session crash:** On the upper left panel find `Runtime` and select `Restart Session`. (Good news: In the second run you won't need to wait for downloads)\n",
    "- **NVIDIA driver error:** Check if the GPU is correctly shown in the `# 0: Preps >>> Verify GPU session`.\n",
    "- **Timeout:** Colab sessions are limited; you may need to restart and rerun.\n",
    "- **Download errors:** If dataset or model download fails due to interruption, delete any existing `datasets/` or `training_sessions/` folders from the loaded git folder and try again.\n",
    "\n",
    "**Note: All repeating cells used for data preparation before different testing types will be hidden using CLI support. Currently this notebook is showing how the underlying config structure and functions works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320a3c0",
   "metadata": {},
   "source": [
    "--- \n",
    "## 0: Preps  \n",
    "Clone repo, install requirements, and verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Verify GPU session\n",
    "#================================================================\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124aa6e",
   "metadata": {},
   "source": [
    "Please don't continue if `CUDA available: False` and select GPU again and restart session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66985e8a",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Colab Preps (Skip if local build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e64371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Clone Repo (Skip here if you are running locally)\n",
    "#================================================================\n",
    "!git clone https://github.com/Mert-chan/IonoBench --quiet\n",
    "%cd IonoBench\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Preps >>> Download the required libs\n",
    "#================================================================\n",
    "!pip install -r /content/IonoBench/requirements_colab.txt --quiet              # Can take couple of minutes\n",
    "!pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Install cu118 Just incase if pytorch can't find \n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21724f",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1: Dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66ea3a",
   "metadata": {},
   "source": [
    "If chronological split is desired, change to \"chronological\" However for chronological you can only replicate the Paper Results on  `Section 4 Investigating Future Bias in Stratified Split `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40245fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Download the desired IonoBench dataset (Stratified or Chronological split)\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "import os,sys\n",
    "\n",
    "repo_name = \"IonoBench\"  \n",
    "base_path = Path(f\"/content/{repo_name}\")\n",
    "sys.path.append(str(base_path))\n",
    "sys.path.append('./source')\n",
    "sys.path.append('./scripts')\n",
    "from source.myDataFuns import download_dataset\n",
    "download_dataset(\n",
    "    dataset_name=\"stratified\",  # If chronological split is desired, change to \"chronological\" However for chronological you can only replicate the Paper Results on Section 4\n",
    "    base_path=base_path\n",
    "    )     \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3607a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Read dataset\n",
    "#====================================================================================\n",
    "from scripts.data import load_training_data\n",
    "\n",
    "dataDict = load_training_data(\n",
    "                            seq_len=12,                \n",
    "                            pred_horz=12,\n",
    "                            datasplit='stratified',      # Need to be changed according to the datasplit\n",
    "                            features = None,             # Default \"None\" loads all features, otherwise specify a list of features i.e ['F10.7', 'Dst']\n",
    "                            base_path=base_path          # You can also see the full list of features in ~/configs/base.yaml\n",
    "                            )        \n",
    "\n",
    "\n",
    "print(\"\\n\", \"-\" * 20, \"\\n\", dataDict.keys())\n",
    "print(\"You can use the dataDict following dict_keys as you like from this point on.\")\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features\n",
    "#====================================================================================\n",
    "dataDict['OMNI_names']\n",
    "#====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing specific OMNI features (note: all features are normalized to 0-1) \n",
    "#=====================================================================================\n",
    "'''\n",
    "\"normOMNI\" is same order with the \"OMNI_names\"\n",
    "Check the name and access the desired feature using the getOMNIfeature function.\n",
    "'''\n",
    "from source.myDataFuns import getOMNIfeature\n",
    "\n",
    "# To access 'Dst' values \n",
    "print(\"Dst:\", getOMNIfeature(dataDict,\"Dst\")) # Dst values\n",
    "print(\"Year:\", getOMNIfeature(dataDict,\"Year\")) # Year values\n",
    "\n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Ex: Accessing TEC data for specific date range (note: tec data is normalized to 0-1)\n",
    "#======================================================================================\n",
    "from datetime import datetime, timedelta\n",
    "from source.myDataFuns import dateList\n",
    "# Select the date period\n",
    "startDate = datetime(2024, 5, 9, 0, 0)  # Start date\n",
    "endDate = datetime(2024, 5, 9, 22, 0)    # End date (inclusive)\n",
    "\n",
    "# Find idx corresponding to start and end dates\n",
    "startidx = dataDict['dates'].index(startDate)\n",
    "endidx = dataDict['dates'].index(endDate)\n",
    "\n",
    "# Extract TEC data for the selected date range\n",
    "norm_tecData = dataDict['normTEC'][startidx:endidx+1]\n",
    "date_list = dateList(startDate, endDate,timedelta(hours=2)) # TEC maps are 2 hours apart\n",
    "print(\"TEC data shape for the selected date range:\", norm_tecData.shape)\n",
    "print(\"Dates of the selected date range:\", len(date_list))\n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Dataset >>> Reversing the Preprocessing Steps and Visualizing the Original TEC Data\n",
    "#======================================================================================\n",
    "'''\n",
    "To recover original TEC data, the preprocessing steps (Normalization and heliocentric transformation) needed to be reversed. \n",
    "'''\n",
    "from source.myDataFuns import reverseHeliocentric\n",
    "tecData = norm_tecData*(dataDict['maxTEC'] - dataDict['minTEC']) + dataDict['minTEC']  # Reverse normalization\n",
    "org_tecData = reverseHeliocentric(tecData, date_list)\n",
    "\n",
    "from source.myVisualFuns import makeComparison_Anim\n",
    "from IPython.display import HTML\n",
    "# Define the plot titles for the animation\n",
    "plot_titles = {\n",
    "    'main': 'Reverse Heliocentric Transformation',\n",
    "    'subplot1': 'Real VTEC IGS',\n",
    "    'subplot2': 'Longitude-Shifted VTEC IGS',\n",
    "    'colorbar': 'TECU (10$^{16}$ e/m$^2$)'\n",
    "}\n",
    "\n",
    "# Create a comparison animation of the original and transformed TEC data\n",
    "'''\n",
    "This is for demonstration purposes. Reversing the effect of the heliocentric transformation is shown in below animation\n",
    "Right side shows after reverse heliocentric transformation, left side shows dataset TEC data before transformation.\n",
    "'''\n",
    "anim = makeComparison_Anim(\n",
    "    data1=org_tecData,\n",
    "    data2=tecData,\n",
    "    date_list=date_list,\n",
    "    titles=plot_titles,\n",
    "    show_metrics=False\n",
    ")\n",
    "HTML(anim.to_jshtml())  \n",
    "#======================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b00ee",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2: Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8601b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Download the desired Trained Model (SimVP2,SwinLSTM, DCNN etc.) from Hugging Face Hub\n",
    "#==================================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "from source.myDataFuns import download_model_folder\n",
    "\n",
    "data_path = Path(base_path, \"training_sessions\")         # ..~/DemoRepo/training_sessions \n",
    "data_path.mkdir(parents=True, exist_ok=True)             # Create new folder when model folder is downloaded.\n",
    "\n",
    "                          #\n",
    "download_model_folder(                                   # This function automatically downloads the paper's model folder from Hugging Face Hub.\n",
    "                    model_name = \"SimVPv2\",              # Change to \"DCNN\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                    base_path = base_path\n",
    "                    )    \n",
    "#==================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Load the model configurations\n",
    "#==================================================================================\n",
    "from scripts.loadConfigs import load_configs\n",
    "\n",
    "'''\n",
    "Configs uses base => model => mode => CLI override hierarchy.\n",
    "You can change the model and mode to load different configurations.\n",
    "For example, change \"SimVPv2\" to \"DCNN121\", \"SwinLSTM\" etc. to load different model configurations.\n",
    "load_configs will return a merged configurations of the base, model, and mode.\n",
    "'''\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # Change to \"DCNN121\", \"SwinLSTM\", \"SimVPv2_Chrono\" for other models.\n",
    "                 mode = \"test\",         \n",
    "                 split = \"stratified\",  \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "#=================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ceca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Models and Configs >>> Can check the keys of the config dictionary\n",
    "#================================================================\n",
    "print(cfgs.keys())\n",
    "print(cfgs.data.keys())\n",
    "print(cfgs.model.keys())\n",
    "print(cfgs.data.data_split)\n",
    "print(cfgs.model)\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795a207",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3: Testing Trained Models \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fa3bb",
   "metadata": {},
   "source": [
    "\n",
    "### 3a: Loading Pre-trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021206b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model>>> Prepare the data and build the model     \n",
    "#=================================================================\n",
    "\n",
    "from scripts.registry import build_model\n",
    "from scripts.data import prepare_raw\n",
    "\n",
    "# Prepare the data for testing\n",
    "data = prepare_raw(cfgs)               # This fun Wraps load_training_data & patches cfg with shape/min-max info.\n",
    "B = cfgs.test.batch_size               # batch size from YAML\n",
    "T = cfgs.data.seq_len                  # input sequence length\n",
    "C = cfgs.data.num_omni + 1             # OMNI scalars + TEC map channel\n",
    "H = cfgs.data.H                        # height (set inside prepare_raw)\n",
    "W = cfgs.data.W                        # width  (set inside prepare_raw)\n",
    "cfgs.test.input_names = data['OMNI_names']          # This sets the input names for test logging file\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise CPU\n",
    "# Build the model\n",
    "cfgs.model.input_shape = (T, C, H, W)  # Set input shape for the model\n",
    "model = build_model(cfg = cfgs, base_path=base_path, device=device)           # Build the model with the configurations\n",
    "print(f\"Batch size: {B}, T: {T}, C: {C}, H: {H}, W: {W}\")\n",
    "#=================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Summary of the model   \n",
    "#=================================================================\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=((B, C, T, H, W),(B, cfgs.data.pred_horz, H, W)))\n",
    "#================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e83f4",
   "metadata": {},
   "source": [
    "\n",
    "!!!! **Before loading the weights to build model**  Change the Path of **torch.load** to the path of your downloaded model checkpoint folder if you are trying DCNN or SwinLSTM. (Default is set to SimVPv2)\n",
    "You can find the checkpoint file in the downloaded model folder under `training_sessions/{modelName}` as .... `\"NameofTheSession\"_best_checkpoint_\"yyyymmdd\"_\"hhmm\" `\n",
    "To download new pre-trained model, refer back to the `2: Models and Configs >>> Download the desired Trained Model` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a: Loading Pre-trained Model >>> Load the model weights from a checkpoint file\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import DDPtoSingleGPU\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    r'/content/IonoBench/training_sessions/SimVPv2/SimVP_stratifiedSplit_Allfeatures_best_checkpoint_20250320_1401.pth',            # <= Copy inside\n",
    "    weights_only=True)\n",
    "model.load_state_dict(DDPtoSingleGPU(checkpoint[\"model_state_dict\"]))   # Load the model state dict If DDP(Multiple GPU) was used get rid of Module prefix\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913f21",
   "metadata": {},
   "source": [
    "--- \n",
    "### 3b: Default Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa314fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**!!! Disclaimer !!!**  \n",
    "The paper’s experiments were primarily trained and tested using **4 GPUs with Distributed Data Parallel (DDP)**.  \n",
    "DDP introduces some non-determinism, even with fixed random seeds, due to differences in data shuffling and the order of floating-point operations across GPUs.\n",
    "\n",
    "This notebook uses a **single GPU**, which may lead to minor variations between the metrics generated here and those reported in the paper\n",
    "\n",
    "These differences are **statistically negligible** and can be validated by comparing with the reported results.  \n",
    "The key point is that these differences will not affect the paper's overall findings or the relative performance ranking of the models.\n",
    "\n",
    "For bit-for-bit reproducibility of your own experiments, it is essential to use a fixed hardware configuration (e.g., the same number of GPUs) for both training and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Setup the data loaders for testing\n",
    "#================================================================\n",
    "from scripts.data import make_default_loaders\n",
    "loaders = make_default_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "len(loaders[\"train\"]), len(loaders[\"valid\"]), len(loaders[\"test\"])\n",
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b: Default Test >>> Test the model (Default: test set) (RUN TIME: >~20 min)\n",
    "#================================================================\n",
    "from source.myTrainFuns import IonoTester\n",
    "\n",
    "# FOR DEFAULT TESTING UNCOMMENT BELOW\n",
    "#================================================================\n",
    "'''\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = False              # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = IonoTester(model, loaders['test'], device=device, config=cfgs).test() \n",
    "'''\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba3a0a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4: Solar and Storm Analysis\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08ca93",
   "metadata": {},
   "source": [
    "\n",
    "### 4a: Solar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88217c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar and Storm Analysis >>> Solar Loaders \n",
    "#====================================================================================\n",
    "from scripts.data import make_solar_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # <= change to \"DCNN121\", \"SwinLSTM\" for other models. (But the loaded model should match the model name)\n",
    "                 mode = \"solar\",     # <= The testing type is set to \"solar\" to load solar configurations dynmaically.\n",
    "                 split = \"stratified\",  \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_solar_loaders(cfg = cfgs, base_path=base_path, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a: Solar Analysis >>> Analysis Function (Testing on Solar intensity classes: very weak, weak, moderate, intense) (RUN TIME: >~20 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import SolarAnalysis\n",
    "\n",
    "# FOR SOLAR ANALYSIS UNCOMMENT BELOW\n",
    "#================================================================\n",
    "'''\n",
    "cfgs.session.name = \"SimVPv2_test\"  # Writes on top of the previous test file if the session name is the same.\n",
    "cfgs.test.save_results = True       # Appends the test results to a log file (txt) that exists in the training_sessions folder under the session name. (if not exists, creates a new one) \n",
    "cfgs.test.save_raw = False          # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz. (Per solar class)\n",
    "solarDict = SolarAnalysis(model, data, loaders, device=device, cfg=cfgs).run()\n",
    "'''\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820351b4",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b: Storm Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Storm Loaders\n",
    "#====================================================================================\n",
    "from scripts.data import make_storm_loaders\n",
    "\n",
    "if \"loaders\" in locals():\n",
    "    del loaders, data, cfgs             # To clear memory before loading solar loaders\n",
    "    \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfgs = load_configs(\n",
    "                 model = \"SimVPv2\",  # <= change to \"DCNN121\", \"SwinLSTM\" for other models. (But the loaded model should match the model name)\n",
    "                 mode = \"storm\",     # <= changed to storm to load storm configurations dynamically.\n",
    "                 split = \"stratified\", \n",
    "                 base_path= base_path\n",
    "                 )\n",
    "cfgs.paths.base_dir = base_path\n",
    "data = prepare_raw(cfgs)                # Prepare the data (again neeeded for setting important parameters to config inside) \n",
    "loaders = make_storm_loaders(cfg = cfgs, d = data)                # Make default loaders for training, validation, and test sets.\n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00951699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b: Storm Analysis >>> Analysis Function (Testing on Storm events) (RUN TIME: ~1-3 Mins)\n",
    "#====================================================================================\n",
    "from source.myTrainFuns import StormAnalysis\n",
    "\n",
    "cfgs.test.save_results = True           # Save the test results to a log file (txt).\n",
    "cfgs.test.save_raw = True               # Save the raw test results (Prediction Dates, TEC predictions and dedicated truth maps) to npz.\n",
    "cfgs.session.name = \"SimVPv2_test\"      # Write a session name for the test results. New name will create a new folder in the ~/IonoBenchv1/training_sessions/\n",
    "testDict = StormAnalysis(model, data, cfgs, loaders,device).run() \n",
    "#===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c0de",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5: Storm Predictions and Residuals\n",
    "---\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91156d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "from source.myVisualFuns import spatialComparison_Anim\n",
    "\n",
    "# Example Storm Animation (Storm 2, 2001-11-06)\n",
    "npz_path = base_path / \"training_sessions\" / cfgs.session.name / \"storms\" / \"test_raw_storm_2_2001-11-06.npz\"       # You can change visualize all storms by changing npz_path to the desired storm npz file\n",
    "# Note: npz_path can be found in the ~/IonoBenchv1/training_sessions/SimVPv2_test/storms/ folder after running the StormAnalysis function above.\n",
    "\n",
    "anim = spatialComparison_Anim(\n",
    "    npz_path=npz_path,\n",
    "    dataDict=dataDict,\n",
    "    cfgs=cfgs,\n",
    "    n_start=33,             # Starting storm index (0-72, default 0) 33 is start of main phase +-6h\n",
    "    n_end=40,               # Ending storm index (0-72, default None, uses all) 40 is end of main phase +-6h\n",
    "    horizon=6,              # 0-11 steps: 0 → 2h, 5 → 12h, etc.\n",
    "    interval_ms=300,\n",
    "    save=True              # set False if you only want inline display\n",
    ")\n",
    "\n",
    "HTML(anim.to_jshtml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ionobench_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
